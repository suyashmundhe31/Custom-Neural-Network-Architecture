{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Market Research Chatbot - Neural Network from Scratch"
      ],
      "metadata": {
        "id": "RdZwH4Wp1RMy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLV861vc1FbM",
        "outputId": "7336d367-b654-48ed-b17a-a95c3453b863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "All dependencies installed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install nltk pandas numpy tensorflow scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Input, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "print(\"All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your CSV data\n",
        "df = pd.read_csv('/content/sample_data/market_research_chatbot_dataset.csv')\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VaJghkn1eu7",
        "outputId": "add22e6b-b754-481d-a482-c2b2e1b85880"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Dataset shape: (750, 2)\n",
            "\n",
            "Sample data:\n",
            "                                            Question  \\\n",
            "0               Name some leading startups in AI/ML.   \n",
            "1               List major players active in Edtech.   \n",
            "2           Who are major AI/ML market participants?   \n",
            "3                 What occupations use Fintech most?   \n",
            "4  What growth is forecasted for the Fintech sector?   \n",
            "\n",
            "                                              Answer  \n",
            "0  Big players include OpenAI, Google DeepMind, a...  \n",
            "1  Top edtech players include Coursera, Udemy, an...  \n",
            "2  Big players include OpenAI, Google DeepMind, a...  \n",
            "3  Fintech adoption is highest among millennials ...  \n",
            "4      The fintech sector may grow to $460B by 2025.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TEXT PREPROCESSING =====\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"Tokenize text using NLTK\"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                 if token not in self.stop_words and len(token) > 1]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        cleaned_text = self.clean_text(text)\n",
        "        tokens = self.tokenize_text(cleaned_text)\n",
        "        return tokens\n",
        "\n",
        "# Preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Preprocess questions and answers\n",
        "print(\"Preprocessing text data...\")\n",
        "questions_processed = []\n",
        "answers_processed = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    if pd.notna(row['Question']) and pd.notna(row['Answer']):\n",
        "        q_tokens = preprocessor.preprocess(row['Question'])\n",
        "        a_tokens = preprocessor.preprocess(row['Answer'])\n",
        "\n",
        "        questions_processed.append(q_tokens)\n",
        "        answers_processed.append(a_tokens)\n",
        "\n",
        "print(f\"Text preprocessing completed! Processed {len(questions_processed)} pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y97lGN_y1mcr",
        "outputId": "abc96f67-b578-40a3-aa89-f4b7f77d6214"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text data...\n",
            "Text preprocessing completed! Processed 750 pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== VOCABULARY BUILDING =====\n",
        "\n",
        "def build_vocabulary(tokenized_texts, min_freq=2):\n",
        "    \"\"\"Build vocabulary from tokenized texts\"\"\"\n",
        "    all_words = []\n",
        "    for tokens in tokenized_texts:\n",
        "        all_words.extend(tokens)\n",
        "\n",
        "    vocab_counter = Counter(all_words)\n",
        "    vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + [\n",
        "        word for word, count in vocab_counter.most_common() if count >= min_freq\n",
        "    ]\n",
        "\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "    return vocab, word_to_idx, idx_to_word\n",
        "\n",
        "# Build vocabulary from questions and answers\n",
        "all_texts = questions_processed + answers_processed\n",
        "vocab, word_to_idx, idx_to_word = build_vocabulary(all_texts, min_freq=2)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Sample vocabulary: {vocab[:20]}\")"
      ],
      "metadata": {
        "id": "0It_hTdo1vGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db5f06d-37f9-4b5c-e512-ac9816202b3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 258\n",
            "Sample vocabulary: ['<PAD>', '<UNK>', '<START>', '<END>', 'edtech', 'aiml', 'fintech', 'include', 'model', 'ecommerce', 'healthtech', 'trend', '2025', 'user', 'firm', 'market', 'player', 'use', 'key', 'whats']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== GLOVE EMBEDDINGS =====\n",
        "\n",
        "def download_glove():\n",
        "    \"\"\"Download GloVe embeddings\"\"\"\n",
        "    import urllib.request\n",
        "    import zipfile\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists('glove.6B.100d.txt'):\n",
        "        print(\"Downloading GloVe embeddings...\")\n",
        "        try:\n",
        "            url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "            urllib.request.urlretrieve(url, 'glove.6B.zip')\n",
        "\n",
        "            with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "                zip_ref.extract('glove.6B.100d.txt')\n",
        "\n",
        "            os.remove('glove.6B.zip')\n",
        "            print(\"GloVe embeddings downloaded!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading GloVe: {e}\")\n",
        "            print(\"Will use random embeddings instead.\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def load_glove_embeddings(vocab, embedding_dim=100):\n",
        "    \"\"\"Load GloVe embeddings\"\"\"\n",
        "    glove_available = download_glove()\n",
        "\n",
        "    embeddings_index = {}\n",
        "\n",
        "    if glove_available:\n",
        "        with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    values = line.split()\n",
        "                    word = values[0]\n",
        "                    coefs = np.asarray(values[1:], dtype='float32')\n",
        "                    embeddings_index[word] = coefs\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        print(f\"Found {len(embeddings_index)} word vectors in GloVe.\")\n",
        "\n",
        "    # Embedding matrix\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "\n",
        "    for word, idx in word_to_idx.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[idx] = embedding_vector\n",
        "        else:\n",
        "            # Random initialization for unknown words\n",
        "            embedding_matrix[idx] = np.random.normal(0, 0.1, embedding_dim)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load GloVe embeddings\n",
        "EMBEDDING_DIM = 100\n",
        "embedding_matrix = load_glove_embeddings(vocab, EMBEDDING_DIM)\n",
        "print(\"Embeddings loaded successfully!\")"
      ],
      "metadata": {
        "id": "w5-WsLaH1vDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7673849-0ba4-4ffd-b773-48817b112b6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "GloVe embeddings downloaded!\n",
            "Found 400000 word vectors in GloVe.\n",
            "Embeddings loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== DATA ENCODING =====\n",
        "\n",
        "def encode_sequences(tokenized_texts, word_to_idx, max_length=None):\n",
        "    \"\"\"Convert tokenized texts to sequences of indices\"\"\"\n",
        "    sequences = []\n",
        "    for tokens in tokenized_texts:\n",
        "        sequence = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    if max_length is None:\n",
        "        max_length = max(len(seq) for seq in sequences) if sequences else 10\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "    return padded_sequences, max_length\n",
        "\n",
        "# Sequence lengths based on data distribution\n",
        "question_lengths = [len(q) for q in questions_processed]\n",
        "answer_lengths = [len(a) for a in answers_processed]\n",
        "\n",
        "# 95th percentile to determine max lengths\n",
        "MAX_QUESTION_LENGTH = min(30, int(np.percentile(question_lengths, 95))) if question_lengths else 20\n",
        "MAX_ANSWER_LENGTH = min(60, int(np.percentile(answer_lengths, 95))) if answer_lengths else 40\n",
        "\n",
        "print(f\"Max question length: {MAX_QUESTION_LENGTH}\")\n",
        "print(f\"Max answer length: {MAX_ANSWER_LENGTH}\")\n",
        "\n",
        "# Encode questions and answers\n",
        "questions_encoded, _ = encode_sequences(questions_processed, word_to_idx, MAX_QUESTION_LENGTH)\n",
        "answers_encoded, _ = encode_sequences(answers_processed, word_to_idx, MAX_ANSWER_LENGTH)\n",
        "\n",
        "print(f\"Questions encoded shape: {questions_encoded.shape}\")\n",
        "print(f\"Answers encoded shape: {answers_encoded.shape}\")"
      ],
      "metadata": {
        "id": "LxS-jtMz1swE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3426a4a-ceb2-43b0-e780-7e6f5aaa333a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max question length: 5\n",
            "Max answer length: 7\n",
            "Questions encoded shape: (750, 5)\n",
            "Answers encoded shape: (750, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== NEURAL NETWORK ARCHITECTURE =====\n",
        "\n",
        "class MarketResearchChatbot:\n",
        "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, max_question_length, max_answer_length):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.max_question_length = max_question_length\n",
        "        self.max_answer_length = max_answer_length\n",
        "        self.model = None\n",
        "        self.encoder_model = None\n",
        "        self.decoder_model = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build LSTM-based seq2seq model\n",
        "\n",
        "        NEURAL NETWORK LAYERS ARCHITECTURE:\n",
        "\n",
        "        1. EMBEDDING LAYER:\n",
        "           - Input: Integer sequences (word indices)\n",
        "           - Output: Dense vectors (GloVe embeddings)\n",
        "           - Parameters: vocab_size x embedding_dim\n",
        "\n",
        "        2. ENCODER LAYERS:\n",
        "           - Bidirectional LSTM (512 units total: 256 forward + 256 backward)\n",
        "           - Dropout: 0.3 (prevents overfitting)\n",
        "           - Recurrent Dropout: 0.3\n",
        "           - Returns: sequences, forward states, backward states\n",
        "\n",
        "        3. DECODER LAYERS:\n",
        "           - LSTM (512 units)\n",
        "           - Dropout: 0.3\n",
        "           - Recurrent Dropout: 0.3\n",
        "           - Returns: sequences and states\n",
        "\n",
        "        4. ATTENTION MECHANISM (Simplified):\n",
        "           - Dense layer for attention weights\n",
        "           - Context vector computation\n",
        "\n",
        "        5. OUTPUT LAYER:\n",
        "           - Dense layer with softmax activation\n",
        "           - Size: vocab_size (probability distribution over vocabulary)\n",
        "        \"\"\"\n",
        "\n",
        "        # ENCODER ARCHITECTURE\n",
        "        encoder_inputs = Input(shape=(self.max_question_length,), name='encoder_input')\n",
        "\n",
        "        # Layer 1: Embedding Layer\n",
        "        embedding_layer = Embedding(\n",
        "            input_dim=self.vocab_size,\n",
        "            output_dim=self.embedding_dim,\n",
        "            weights=[self.embedding_matrix],\n",
        "            trainable=False,\n",
        "            mask_zero=True,\n",
        "            name='embedding_layer'\n",
        "        )\n",
        "\n",
        "        encoder_embedded = embedding_layer(encoder_inputs)\n",
        "\n",
        "        # Layer 2: Bidirectional LSTM Encoder\n",
        "        encoder_lstm = Bidirectional(\n",
        "            LSTM(256, return_sequences=True, return_state=True,\n",
        "                 dropout=0.3, recurrent_dropout=0.3),\n",
        "            name='encoder_bilstm'\n",
        "        )\n",
        "\n",
        "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedded)\n",
        "\n",
        "        # Combine forward and backward states\n",
        "        state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "        # DECODER ARCHITECTURE\n",
        "        decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
        "        decoder_embedded = embedding_layer(decoder_inputs)\n",
        "\n",
        "        # Layer 3: LSTM Decoder\n",
        "        decoder_lstm = LSTM(\n",
        "            512,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3,\n",
        "            name='decoder_lstm'\n",
        "        )\n",
        "\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
        "\n",
        "        # Layer 4: Attention Mechanism\n",
        "        attention = Dense(512, activation='tanh', name='attention_layer')(decoder_outputs)\n",
        "\n",
        "        # Layer 5: Output Dense Layer\n",
        "        decoder_dense = Dense(self.vocab_size, activation='softmax', name='output_layer')\n",
        "        decoder_outputs = decoder_dense(attention)\n",
        "\n",
        "        # Define the training model\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_chatbot')\n",
        "\n",
        "        # Compile with improved optimizer settings\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        # Inference models\n",
        "        # Encoder model for inference\n",
        "        self.encoder_model = Model(encoder_inputs, encoder_states, name='encoder_inference')\n",
        "\n",
        "        # Decoder model for inference\n",
        "        decoder_state_input_h = Input(shape=(512,), name='decoder_state_h')\n",
        "        decoder_state_input_c = Input(shape=(512,), name='decoder_state_c')\n",
        "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "            decoder_embedded, initial_state=decoder_states_inputs)\n",
        "\n",
        "        # Apply attention and output layers\n",
        "        attention_inf = Dense(512, activation='tanh', name='attention_inference')(decoder_outputs)\n",
        "        decoder_outputs = decoder_dense(attention_inf)\n",
        "\n",
        "        decoder_states = [state_h, state_c]\n",
        "\n",
        "        self.decoder_model = Model(\n",
        "            [decoder_inputs] + decoder_states_inputs,\n",
        "            [decoder_outputs] + decoder_states,\n",
        "            name='decoder_inference'\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "# Initialize chatbot\n",
        "chatbot = MarketResearchChatbot(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    embedding_matrix=embedding_matrix,\n",
        "    max_question_length=MAX_QUESTION_LENGTH,\n",
        "    max_answer_length=MAX_ANSWER_LENGTH\n",
        ")\n",
        "\n",
        "model = chatbot.build_model()\n",
        "print(\"Model architecture created!\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NEURAL NETWORK LAYERS SUMMARY:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Embedding Layer: Converts word indices to dense vectors\")\n",
        "print(\"2. Bidirectional LSTM Encoder: Processes input questions\")\n",
        "print(\"3. LSTM Decoder: Generates output answers\")\n",
        "print(\"4. Attention Layer: Focuses on relevant parts of input\")\n",
        "print(\"5. Dense Output Layer: Produces probability distribution over vocabulary\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total parameters: {model.count_params():,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "2VfKNOOz15Ne",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "outputId": "4e46d74b-22a8-4bae-daed-1baa45719cd5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architecture created!\n",
            "\n",
            "============================================================\n",
            "NEURAL NETWORK LAYERS SUMMARY:\n",
            "============================================================\n",
            "1. Embedding Layer: Converts word indices to dense vectors\n",
            "2. Bidirectional LSTM Encoder: Processes input questions\n",
            "3. LSTM Decoder: Generates output answers\n",
            "4. Attention Layer: Focuses on relevant parts of input\n",
            "5. Dense Output Layer: Produces probability distribution over vocabulary\n",
            "============================================================\n",
            "Total parameters: 2,407,370\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"seq2seq_chatbot\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_chatbot\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │     \u001b[38;5;34m25,800\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_bilstm      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m),  │    \u001b[38;5;34m731,136\u001b[0m │ embedding_layer[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ encoder_bilstm[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m1,255,424\u001b[0m │ embedding_layer[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │    \u001b[38;5;34m262,656\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m258\u001b[0m) │    \u001b[38;5;34m132,354\u001b[0m │ attention_layer[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,800</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_bilstm      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">731,136</span> │ embedding_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ encoder_bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,255,424</span> │ embedding_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">132,354</span> │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,407,370\u001b[0m (9.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,370</span> (9.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,381,570\u001b[0m (9.08 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,381,570</span> (9.08 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m25,800\u001b[0m (100.78 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,800</span> (100.78 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TRAINING DATA PREPARATION =====\n",
        "\n",
        "def prepare_training_data(questions_encoded, answers_encoded, word_to_idx):\n",
        "    \"\"\"Prepare training data for seq2seq model\"\"\"\n",
        "    encoder_input_data = questions_encoded\n",
        "\n",
        "    # Decoder input and target data preparation\n",
        "    decoder_input_data = np.zeros((len(answers_encoded), MAX_ANSWER_LENGTH), dtype='int32')\n",
        "    decoder_target_data = np.zeros((len(answers_encoded), MAX_ANSWER_LENGTH), dtype='int32')\n",
        "\n",
        "    for i, answer_seq in enumerate(answers_encoded):\n",
        "        # Find actual length\n",
        "        actual_length = np.count_nonzero(answer_seq)\n",
        "\n",
        "        if actual_length > 0:\n",
        "            # Decoder input: START token + answer\n",
        "            decoder_input_data[i, 0] = word_to_idx['<START>']\n",
        "            if actual_length > 1:\n",
        "                decoder_input_data[i, 1:min(actual_length, MAX_ANSWER_LENGTH-1)] = answer_seq[:min(actual_length-1, MAX_ANSWER_LENGTH-2)]\n",
        "\n",
        "            # Decoder target: answer + END token\n",
        "            decoder_target_data[i, :min(actual_length, MAX_ANSWER_LENGTH-1)] = answer_seq[:min(actual_length, MAX_ANSWER_LENGTH-1)]\n",
        "            if actual_length < MAX_ANSWER_LENGTH:\n",
        "                decoder_target_data[i, min(actual_length, MAX_ANSWER_LENGTH-1)] = word_to_idx['<END>']\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
        "\n",
        "encoder_input_data, decoder_input_data, decoder_target_data = prepare_training_data(\n",
        "    questions_encoded, answers_encoded, word_to_idx)\n",
        "\n",
        "print(\"Training data prepared!\")\n",
        "print(f\"Training samples: {len(encoder_input_data)}\")\n",
        "print(f\"Encoder input shape: {encoder_input_data.shape}\")\n",
        "print(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
        "print(f\"Decoder target shape: {decoder_target_data.shape}\")"
      ],
      "metadata": {
        "id": "s77383wv2Fll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc48429b-caeb-4f16-84d9-87b5630474ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data prepared!\n",
            "Training samples: 750\n",
            "Encoder input shape: (750, 5)\n",
            "Decoder input shape: (750, 7)\n",
            "Decoder target shape: (750, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== MODEL TRAINING =====\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "# Callbacks for better training\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.0001,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "batch_size = min(32, len(encoder_input_data) // 10)\n",
        "epochs = 150 if len(encoder_input_data) > 100 else 50\n",
        "\n",
        "print(f\"Training with batch size: {batch_size}, epochs: {epochs}\")\n",
        "\n",
        "history = model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Model training completed!\")"
      ],
      "metadata": {
        "id": "hKSJxaOj2FbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0660cb36-b22b-4219-9657-f3b951ab7b66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Training with batch size: 32, epochs: 150\n",
            "Epoch 1/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 198ms/step - accuracy: 0.0830 - loss: 4.8611 - val_accuracy: 0.2771 - val_loss: 2.6821 - learning_rate: 0.0010\n",
            "Epoch 2/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step - accuracy: 0.3843 - loss: 2.3305 - val_accuracy: 0.6676 - val_loss: 1.1037 - learning_rate: 0.0010\n",
            "Epoch 3/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6610 - loss: 0.9883 - val_accuracy: 0.6886 - val_loss: 0.5612 - learning_rate: 0.0010\n",
            "Epoch 4/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.7076 - loss: 0.5444 - val_accuracy: 0.7543 - val_loss: 0.3061 - learning_rate: 0.0010\n",
            "Epoch 5/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.7445 - loss: 0.3437 - val_accuracy: 0.7676 - val_loss: 0.2286 - learning_rate: 0.0010\n",
            "Epoch 6/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.7593 - loss: 0.2599 - val_accuracy: 0.7676 - val_loss: 0.1886 - learning_rate: 0.0010\n",
            "Epoch 7/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.7688 - loss: 0.2143 - val_accuracy: 0.7952 - val_loss: 0.1402 - learning_rate: 0.0010\n",
            "Epoch 8/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.7905 - loss: 0.1576 - val_accuracy: 0.8095 - val_loss: 0.1055 - learning_rate: 0.0010\n",
            "Epoch 9/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.7913 - loss: 0.1422 - val_accuracy: 0.8029 - val_loss: 0.1117 - learning_rate: 0.0010\n",
            "Epoch 10/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.8022 - loss: 0.1254 - val_accuracy: 0.8076 - val_loss: 0.0703 - learning_rate: 0.0010\n",
            "Epoch 11/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.8108 - loss: 0.0908 - val_accuracy: 0.8124 - val_loss: 0.0598 - learning_rate: 0.0010\n",
            "Epoch 12/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8166 - loss: 0.0753 - val_accuracy: 0.8276 - val_loss: 0.0312 - learning_rate: 0.0010\n",
            "Epoch 13/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8297 - loss: 0.0469 - val_accuracy: 0.8333 - val_loss: 0.0170 - learning_rate: 0.0010\n",
            "Epoch 14/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8305 - loss: 0.0344 - val_accuracy: 0.8286 - val_loss: 0.0160 - learning_rate: 0.0010\n",
            "Epoch 15/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8302 - loss: 0.0301 - val_accuracy: 0.8333 - val_loss: 0.0122 - learning_rate: 0.0010\n",
            "Epoch 16/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.8302 - loss: 0.0257 - val_accuracy: 0.8333 - val_loss: 0.0110 - learning_rate: 0.0010\n",
            "Epoch 17/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.8301 - loss: 0.0189 - val_accuracy: 0.8333 - val_loss: 0.0048 - learning_rate: 0.0010\n",
            "Epoch 18/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8348 - loss: 0.0219 - val_accuracy: 0.8333 - val_loss: 0.0056 - learning_rate: 0.0010\n",
            "Epoch 19/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8309 - loss: 0.0139 - val_accuracy: 0.8333 - val_loss: 0.0042 - learning_rate: 0.0010\n",
            "Epoch 20/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8348 - loss: 0.0166 - val_accuracy: 0.8333 - val_loss: 0.0065 - learning_rate: 0.0010\n",
            "Epoch 21/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8332 - loss: 0.0125 - val_accuracy: 0.8333 - val_loss: 0.0029 - learning_rate: 0.0010\n",
            "Epoch 22/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.8333 - loss: 0.0087 - val_accuracy: 0.8333 - val_loss: 0.0024 - learning_rate: 0.0010\n",
            "Epoch 23/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8341 - loss: 0.0086 - val_accuracy: 0.8333 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 24/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8327 - loss: 0.0123 - val_accuracy: 0.8324 - val_loss: 0.0052 - learning_rate: 0.0010\n",
            "Epoch 25/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8321 - loss: 0.0094 - val_accuracy: 0.8333 - val_loss: 0.0025 - learning_rate: 0.0010\n",
            "Epoch 26/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8325 - loss: 0.0093 - val_accuracy: 0.8333 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 27/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.8334 - loss: 0.0056 - val_accuracy: 0.8333 - val_loss: 0.0013 - learning_rate: 0.0010\n",
            "Epoch 28/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.8328 - loss: 0.0051 - val_accuracy: 0.8333 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 29/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8358 - loss: 0.0070 - val_accuracy: 0.8333 - val_loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 30/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8344 - loss: 0.0047 - val_accuracy: 0.8333 - val_loss: 0.0010 - learning_rate: 0.0010\n",
            "Epoch 31/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8356 - loss: 0.0047 - val_accuracy: 0.8333 - val_loss: 9.1883e-04 - learning_rate: 0.0010\n",
            "Epoch 32/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8331 - loss: 0.0059 - val_accuracy: 0.8324 - val_loss: 0.0023 - learning_rate: 0.0010\n",
            "Epoch 33/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8301 - loss: 0.0146 - val_accuracy: 0.8324 - val_loss: 0.0104 - learning_rate: 0.0010\n",
            "Epoch 34/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8333 - loss: 0.0139 - val_accuracy: 0.8333 - val_loss: 0.0012 - learning_rate: 0.0010\n",
            "Epoch 35/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.8340 - loss: 0.0079 - val_accuracy: 0.8333 - val_loss: 0.0018 - learning_rate: 0.0010\n",
            "Epoch 36/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8354 - loss: 0.0113\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8353 - loss: 0.0112 - val_accuracy: 0.8333 - val_loss: 0.0011 - learning_rate: 0.0010\n",
            "Epoch 37/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8324 - loss: 0.0050 - val_accuracy: 0.8333 - val_loss: 7.9088e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 38/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8334 - loss: 0.0037 - val_accuracy: 0.8333 - val_loss: 6.8155e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 39/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8373 - loss: 0.0029 - val_accuracy: 0.8333 - val_loss: 6.1496e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 40/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.8365 - loss: 0.0024 - val_accuracy: 0.8333 - val_loss: 5.5184e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 41/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.8338 - loss: 0.0019 - val_accuracy: 0.8333 - val_loss: 5.1336e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 42/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8366 - loss: 0.0021 - val_accuracy: 0.8333 - val_loss: 4.8559e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 43/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8362 - loss: 0.0021 - val_accuracy: 0.8333 - val_loss: 4.7735e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 44/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8340 - loss: 0.0020 - val_accuracy: 0.8333 - val_loss: 4.5540e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 45/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8374 - loss: 0.0022\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8372 - loss: 0.0022 - val_accuracy: 0.8333 - val_loss: 4.5235e-04 - learning_rate: 5.0000e-04\n",
            "Epoch 46/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.8314 - loss: 0.0023 - val_accuracy: 0.8333 - val_loss: 4.1353e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 47/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8340 - loss: 0.0022 - val_accuracy: 0.8333 - val_loss: 4.0295e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 48/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8339 - loss: 0.0021 - val_accuracy: 0.8333 - val_loss: 3.8941e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 49/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8317 - loss: 0.0049 - val_accuracy: 0.8333 - val_loss: 4.1284e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 50/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8339 - loss: 0.0019 - val_accuracy: 0.8333 - val_loss: 3.7221e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 51/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8370 - loss: 0.0019\n",
            "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8369 - loss: 0.0019 - val_accuracy: 0.8333 - val_loss: 3.6919e-04 - learning_rate: 2.5000e-04\n",
            "Epoch 52/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.8344 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.5795e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 53/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8337 - loss: 0.0015 - val_accuracy: 0.8333 - val_loss: 3.5395e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 54/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8321 - loss: 0.0015 - val_accuracy: 0.8333 - val_loss: 3.5079e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 55/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8323 - loss: 0.0017 - val_accuracy: 0.8333 - val_loss: 3.4568e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 56/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8376 - loss: 0.0020\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8375 - loss: 0.0020 - val_accuracy: 0.8333 - val_loss: 3.4233e-04 - learning_rate: 1.2500e-04\n",
            "Epoch 57/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8349 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.3994e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 58/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8367 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.3564e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 59/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8350 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.3086e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 60/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8349 - loss: 0.0013 - val_accuracy: 0.8333 - val_loss: 3.3129e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 61/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8347 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.2867e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 62/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8336 - loss: 0.0017 - val_accuracy: 0.8333 - val_loss: 3.2406e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 63/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8349 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.2331e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 64/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.8338 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.2439e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 65/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8302 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.1765e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 66/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.8306 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.1311e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 67/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8345 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 3.1071e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 68/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8329 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.1079e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 69/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8323 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 3.0950e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 70/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8361 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 3.0847e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 71/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.8359 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 3.0514e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 72/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8353 - loss: 0.0013 - val_accuracy: 0.8333 - val_loss: 2.9915e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 73/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8362 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.9502e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 74/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8346 - loss: 0.0013 - val_accuracy: 0.8333 - val_loss: 2.9326e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 75/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.8337 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.9121e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 76/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8380 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.9061e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 77/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.8328 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 2.8850e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 78/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8306 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.8543e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 79/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8372 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 2.8217e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 80/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.8336 - loss: 0.0010 - val_accuracy: 0.8333 - val_loss: 2.7908e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 81/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8321 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.7661e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 82/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.8341 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 2.7450e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 83/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8359 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.7095e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 84/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8335 - loss: 0.0010 - val_accuracy: 0.8333 - val_loss: 2.6782e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 85/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8338 - loss: 0.0013 - val_accuracy: 0.8333 - val_loss: 2.6403e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 86/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8381 - loss: 0.0015 - val_accuracy: 0.8333 - val_loss: 2.6319e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 87/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - accuracy: 0.8324 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 2.6156e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 88/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.8364 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.6078e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 89/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8343 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 2.6206e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 90/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8353 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 2.6017e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 91/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.8340 - loss: 0.0010 - val_accuracy: 0.8333 - val_loss: 2.5623e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 92/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.8339 - loss: 9.9343e-04 - val_accuracy: 0.8333 - val_loss: 2.5120e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 93/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8345 - loss: 8.9784e-04 - val_accuracy: 0.8333 - val_loss: 2.4790e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 94/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8371 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 2.4619e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 95/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8340 - loss: 0.0013 - val_accuracy: 0.8333 - val_loss: 2.4313e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 96/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8343 - loss: 9.5875e-04 - val_accuracy: 0.8333 - val_loss: 2.4116e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 97/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.8353 - loss: 9.3980e-04 - val_accuracy: 0.8333 - val_loss: 2.3879e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 98/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.8365 - loss: 0.0010 - val_accuracy: 0.8333 - val_loss: 2.3882e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 99/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8347 - loss: 9.1050e-04 - val_accuracy: 0.8333 - val_loss: 2.3784e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 100/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8363 - loss: 8.5040e-04 - val_accuracy: 0.8333 - val_loss: 2.3535e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 101/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8331 - loss: 9.0107e-04 - val_accuracy: 0.8333 - val_loss: 2.3342e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 102/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8318 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 2.4154e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 103/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8319 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.4224e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 104/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.8338 - loss: 9.8883e-04 - val_accuracy: 0.8333 - val_loss: 2.4197e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 105/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8384 - loss: 0.0012 - val_accuracy: 0.8333 - val_loss: 2.3036e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 106/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8354 - loss: 9.6498e-04 - val_accuracy: 0.8333 - val_loss: 2.2266e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 107/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8359 - loss: 8.4908e-04 - val_accuracy: 0.8333 - val_loss: 2.2105e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 108/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8333 - loss: 8.9183e-04 - val_accuracy: 0.8333 - val_loss: 2.1843e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 109/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.8363 - loss: 8.8733e-04 - val_accuracy: 0.8333 - val_loss: 2.1466e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 110/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8342 - loss: 0.0018 - val_accuracy: 0.8333 - val_loss: 2.1201e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 111/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8336 - loss: 8.8502e-04 - val_accuracy: 0.8333 - val_loss: 2.0905e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 112/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8342 - loss: 7.8298e-04 - val_accuracy: 0.8333 - val_loss: 2.0658e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 113/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8386 - loss: 9.7204e-04 - val_accuracy: 0.8333 - val_loss: 2.0553e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 114/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8338 - loss: 9.0531e-04 - val_accuracy: 0.8333 - val_loss: 2.0787e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 115/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.8347 - loss: 7.4300e-04 - val_accuracy: 0.8333 - val_loss: 2.0953e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 116/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8324 - loss: 0.0015 - val_accuracy: 0.8333 - val_loss: 2.0869e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 117/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8326 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 2.0424e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 118/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8336 - loss: 0.0013 - val_accuracy: 0.8333 - val_loss: 2.0324e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 119/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8324 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 2.0164e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 120/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8341 - loss: 0.0010 - val_accuracy: 0.8333 - val_loss: 1.9685e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 121/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.8306 - loss: 0.0018 - val_accuracy: 0.8333 - val_loss: 2.1596e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 122/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8355 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 2.0555e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 123/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8313 - loss: 9.3399e-04 - val_accuracy: 0.8333 - val_loss: 1.9945e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 124/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.8326 - loss: 0.0010 - val_accuracy: 0.8333 - val_loss: 1.9647e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 125/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8337 - loss: 9.0905e-04 - val_accuracy: 0.8333 - val_loss: 1.9717e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 126/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8355 - loss: 9.2639e-04 - val_accuracy: 0.8333 - val_loss: 1.9079e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 127/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.8362 - loss: 7.5836e-04 - val_accuracy: 0.8333 - val_loss: 1.8739e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 128/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - accuracy: 0.8358 - loss: 7.3720e-04 - val_accuracy: 0.8333 - val_loss: 1.8447e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 129/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8361 - loss: 0.0013 - val_accuracy: 0.8333 - val_loss: 1.8442e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 130/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8357 - loss: 0.0011 - val_accuracy: 0.8333 - val_loss: 1.8027e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 131/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8347 - loss: 7.7520e-04 - val_accuracy: 0.8333 - val_loss: 1.7807e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 132/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.8350 - loss: 8.0399e-04 - val_accuracy: 0.8333 - val_loss: 1.7744e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 133/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8325 - loss: 9.4344e-04 - val_accuracy: 0.8333 - val_loss: 1.7613e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 134/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8337 - loss: 7.2538e-04 - val_accuracy: 0.8333 - val_loss: 1.7488e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 135/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.8344 - loss: 8.0824e-04 - val_accuracy: 0.8333 - val_loss: 1.7215e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 136/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8341 - loss: 7.6669e-04 - val_accuracy: 0.8333 - val_loss: 1.7046e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 137/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.8368 - loss: 8.1236e-04 - val_accuracy: 0.8333 - val_loss: 1.6846e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 138/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8311 - loss: 7.5568e-04 - val_accuracy: 0.8333 - val_loss: 1.6974e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 139/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8347 - loss: 6.9917e-04 - val_accuracy: 0.8333 - val_loss: 1.6644e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 140/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8332 - loss: 7.1607e-04 - val_accuracy: 0.8333 - val_loss: 1.6364e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 141/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8334 - loss: 0.0014 - val_accuracy: 0.8333 - val_loss: 1.6995e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 142/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8331 - loss: 7.7694e-04 - val_accuracy: 0.8333 - val_loss: 1.6969e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 143/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.8317 - loss: 7.6015e-04 - val_accuracy: 0.8333 - val_loss: 1.6044e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 144/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.8347 - loss: 7.7029e-04 - val_accuracy: 0.8333 - val_loss: 1.5970e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 145/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.8341 - loss: 6.2016e-04 - val_accuracy: 0.8333 - val_loss: 1.5904e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 146/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.8336 - loss: 7.0666e-04 - val_accuracy: 0.8333 - val_loss: 1.5702e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 147/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.8351 - loss: 7.0083e-04 - val_accuracy: 0.8333 - val_loss: 1.5324e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 148/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 132ms/step - accuracy: 0.8320 - loss: 8.6230e-04 - val_accuracy: 0.8333 - val_loss: 1.5096e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 149/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.8320 - loss: 5.5033e-04 - val_accuracy: 0.8333 - val_loss: 1.4969e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 150/150\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8369 - loss: 6.8693e-04 - val_accuracy: 0.8333 - val_loss: 1.4876e-04 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 150.\n",
            "Model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== INFERENCE FUNCTIONS =====\n",
        "\n",
        "def similarity_search(query_tokens, questions_processed, threshold=0.2):\n",
        "    \"\"\"Find most similar question using token overlap\"\"\"\n",
        "    best_match_idx = -1\n",
        "    best_score = 0\n",
        "\n",
        "    query_set = set(query_tokens)\n",
        "\n",
        "    for i, q_tokens in enumerate(questions_processed):\n",
        "        q_set = set(q_tokens)\n",
        "\n",
        "        intersection = len(query_set.intersection(q_set))\n",
        "        union = len(query_set.union(q_set))\n",
        "\n",
        "        if union > 0:\n",
        "            score = intersection / union\n",
        "            if score > best_score and score >= threshold:\n",
        "                best_score = score\n",
        "                best_match_idx = i\n",
        "\n",
        "    return best_match_idx, best_score\n",
        "\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model, word_to_idx, idx_to_word, max_decoder_seq_length):\n",
        "    \"\"\"Decode a sequence using the trained model\"\"\"\n",
        "    try:\n",
        "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = word_to_idx['<START>']\n",
        "\n",
        "        stop_condition = False\n",
        "        decoded_sentence = []\n",
        "        max_iterations = max_decoder_seq_length\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_word = idx_to_word.get(sampled_token_index, '<UNK>')\n",
        "\n",
        "            if sampled_word in ['<END>', '<PAD>'] or len(decoded_sentence) >= max_decoder_seq_length:\n",
        "                break\n",
        "\n",
        "            if sampled_word != '<UNK>':\n",
        "                decoded_sentence.append(sampled_word)\n",
        "\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            states_value = [h, c]\n",
        "\n",
        "        return ' '.join(decoded_sentence)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in decode_sequence: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_chatbot_response(user_input):\n",
        "    \"\"\"Get response from the chatbot\"\"\"\n",
        "    try:\n",
        "        # Preprocess user input\n",
        "        user_tokens = preprocessor.preprocess(user_input)\n",
        "\n",
        "        if not user_tokens:\n",
        "            return \"I'm sorry, I couldn't understand your question. Could you please rephrase it?\"\n",
        "\n",
        "        # First, Similarity search\n",
        "        match_idx, similarity_score = similarity_search(user_tokens, questions_processed, threshold=0.2)\n",
        "\n",
        "        if match_idx != -1 and similarity_score > 0.4:\n",
        "            return df.iloc[match_idx]['Answer']\n",
        "\n",
        "        # No good match found then neural network prediction\n",
        "        user_sequence = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in user_tokens]\n",
        "        user_padded = pad_sequences([user_sequence], maxlen=MAX_QUESTION_LENGTH, padding='post')\n",
        "\n",
        "        response = decode_sequence(\n",
        "            user_padded,\n",
        "            chatbot.encoder_model,\n",
        "            chatbot.decoder_model,\n",
        "            word_to_idx,\n",
        "            idx_to_word,\n",
        "            MAX_ANSWER_LENGTH\n",
        "        )\n",
        "\n",
        "        # If response is too short or empty, fallback\n",
        "        if len(response.strip()) < 5:\n",
        "            if match_idx != -1:\n",
        "                return df.iloc[match_idx]['Answer']\n",
        "            else:\n",
        "                return \"I'm sorry, I couldn't find relevant information for your query. Could you try rephrasing your question?\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_chatbot_response: {e}\")\n",
        "        return \"I'm sorry, I encountered an error processing your request. Please try again.\""
      ],
      "metadata": {
        "id": "dzWzKEIl2FUM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TESTING THE CHATBOT =====\n",
        "\n",
        "print(\"CHATBOT TESTING WITH Q&A PAIRS\")\n",
        "\n",
        "# Test\n",
        "test_queries = [\n",
        "    \"What is the market size of edtech?\",\n",
        "    \"How big is the fintech market?\",\n",
        "    \"Tell me about healthtech trends\",\n",
        "    \"What are startup funding trends?\",\n",
        "    \"How to validate a business idea?\",\n",
        "    \"What are the key metrics for SaaS startups?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nUser: {query}\")\n",
        "    response = get_chatbot_response(query)\n",
        "    print(f\"Bot: {response}\")\n"
      ],
      "metadata": {
        "id": "5pMOCebM2RyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f4a15d-4abd-4c58-d2e8-fb265c32c144"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHATBOT TESTING WITH Q&A PAIRS\n",
            "\n",
            "User: What is the market size of edtech?\n",
            "Bot: The edtech market is expected to reach $404B by 2025.\n",
            "\n",
            "User: How big is the fintech market?\n",
            "Bot: Notable players are PayPal, Stripe, and Robinhood.\n",
            "\n",
            "User: Tell me about healthtech trends\n",
            "Bot: Trends include telemedicine and wearable health devices.\n",
            "\n",
            "User: What are startup funding trends?\n",
            "Bot: directtoconsumer top major 460b expected fast competitor\n",
            "\n",
            "User: How to validate a business idea?\n",
            "Bot: health major open aidriven aidriven student gamification\n",
            "\n",
            "User: What are the key metrics for SaaS startups?\n",
            "Bot: alibaba alibaba follow clinic clinic healthcare gamification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== INTERACTIVE CHATBOT =====\n",
        "\n",
        "def interactive_chatbot():\n",
        "    \"\"\"Interactive chatbot session\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MARKET RESEARCH CHATBOT - 751 Q&A PAIRS\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Ask me anything about market research, startups, and business!\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "            print(\"Bot: Thank you for using the Market Research Chatbot. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input:\n",
        "            response = get_chatbot_response(user_input)\n",
        "            print(f\"Bot: {response}\")\n",
        "        else:\n",
        "            print(\"Bot: Please enter a valid question.\")\n",
        "\n",
        "print(\"\\nChatbot is ready! Starting interactive session...\")\n",
        "interactive_chatbot()"
      ],
      "metadata": {
        "id": "sd-dQxi82XJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f688669-28ed-4ca7-9045-ddc1d872e8b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chatbot is ready! Starting interactive session...\n",
            "\n",
            "============================================================\n",
            "MARKET RESEARCH CHATBOT - 751 Q&A PAIRS\n",
            "============================================================\n",
            "Ask me anything about market research, startups, and business!\n",
            "Type 'quit' to exit.\n",
            "------------------------------------------------------------\n",
            "\n",
            "You: What is the market size of the AI/ML\n",
            "Bot: AI/ML is estimated to hit $500B by 2025.\n",
            "\n",
            "You: Tell me about AI/ML\n",
            "Bot: AI/ML is estimated to hit $500B by 2025.\n",
            "\n",
            "You: which match is today in IPL\n",
            "Bot: top top goto aidriven millennials estimated estimated\n",
            "\n",
            "You: who is virat kohli\n",
            "Bot: projected model blockchain expected future forecasted forecasted\n",
            "\n",
            "You: What's the go-to business model in AI/ML\n",
            "Bot: AI/ML firms use licensing, APIs, and consulting models.\n",
            "\n",
            "You: How much is the E-commerce sector worth\n",
            "Bot: E-commerce could reach $6.3T globally by 2025.\n",
            "\n",
            "You: Which sectors consume Fintech solutions the most?\n",
            "Bot: Fintech adoption is highest among millennials and Gen Z.\n",
            "\n",
            "You: tell me something about music\n",
            "Bot: popular top 460b 460b aidriven market player\n",
            "\n",
            "You: quit\n",
            "Bot: Thank you for using the Market Research Chatbot. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SAVE MODEL AND COMPONENTS =====\n",
        "\n",
        "print(\"\\nSaving model and components...\")\n",
        "\n",
        "# Save the complete chatbot models using modern Keras format\n",
        "print(\"Saving training model...\")\n",
        "model.save('market_research_chatbot_751_model.keras')\n",
        "\n",
        "print(\"Saving inference models...\")\n",
        "# Save encoder and decoder models for inference\n",
        "chatbot.encoder_model.save('market_research_encoder_model.keras')\n",
        "chatbot.decoder_model.save('market_research_decoder_model.keras')\n",
        "\n",
        "# Save preprocessor as a separate component\n",
        "preprocessor_data = {\n",
        "    'stop_words': list(preprocessor.stop_words),\n",
        "    'lemmatizer_class': type(preprocessor.lemmatizer).__name__\n",
        "}\n",
        "\n",
        "# Save all components including preprocessor\n",
        "components_to_save = {\n",
        "    'vocab': vocab,\n",
        "    'word_to_idx': word_to_idx,\n",
        "    'idx_to_word': idx_to_word,\n",
        "    'questions_processed': questions_processed,\n",
        "    'answers_processed': answers_processed,\n",
        "    'df': df,\n",
        "    'max_question_length': MAX_QUESTION_LENGTH,\n",
        "    'max_answer_length': MAX_ANSWER_LENGTH,\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'vocab_size': len(vocab),\n",
        "    'embedding_matrix': embedding_matrix,\n",
        "    'preprocessor_data': preprocessor_data\n",
        "}\n",
        "\n",
        "# Save components with protocol 4 for better compatibility\n",
        "with open('chatbot_751_components.pkl', 'wb') as f:\n",
        "    pickle.dump(components_to_save, f, protocol=4)\n",
        "\n",
        "# Save model architecture as JSON for reconstruction\n",
        "model_config = model.to_json()\n",
        "with open('model_architecture.json', 'w') as f:\n",
        "    f.write(model_config)\n",
        "\n",
        "print(\"Model and components saved successfully!\")\n",
        "print(\"\\nFiles saved:\")\n",
        "print(\"- market_research_chatbot_751_model.keras (Complete training model)\")\n",
        "print(\"- market_research_encoder_model.keras (Encoder for inference)\")\n",
        "print(\"- market_research_decoder_model.keras (Decoder for inference)\")\n",
        "print(\"- chatbot_751_components.pkl (All components and data)\")\n",
        "print(\"- model_architecture.json (Model architecture)\")\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"- Total training samples: {len(encoder_input_data)}\")\n",
        "print(f\"- Vocabulary size: {len(vocab)}\")\n",
        "print(f\"- Total model parameters: {model.count_params():,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SETUP COMPLETED SUCCESSFULLY!\")\n",
        "print(\"Your custom neural network chatbot is fully saved!\")\n",
        "print(\"All models and components are ready for future use.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "DxPxWZ_H2YCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7513599-341c-45c0-f64f-f3742531e50a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving model and components...\n",
            "Saving training model...\n",
            "Saving inference models...\n",
            "Model and components saved successfully!\n",
            "\n",
            "Files saved:\n",
            "- market_research_chatbot_751_model.keras (Complete training model)\n",
            "- market_research_encoder_model.keras (Encoder for inference)\n",
            "- market_research_decoder_model.keras (Decoder for inference)\n",
            "- chatbot_751_components.pkl (All components and data)\n",
            "- model_architecture.json (Model architecture)\n",
            "\n",
            "Model Statistics:\n",
            "- Total training samples: 750\n",
            "- Vocabulary size: 258\n",
            "- Total model parameters: 2,407,370\n",
            "\n",
            "============================================================\n",
            "SETUP COMPLETED SUCCESSFULLY!\n",
            "Your custom neural network chatbot is fully saved!\n",
            "All models and components are ready for future use.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== LOADING FUNCTION FOR FUTURE USE =====\n",
        "\n",
        "def load_saved_chatbot():\n",
        "    \"\"\"\n",
        "    Complete function to load the saved chatbot for future use.\n",
        "    Use this code when you want to load your saved model in a new session.\n",
        "    \"\"\"\n",
        "    loading_code = '''\n",
        "# Complete code to load your saved custom neural network chatbot\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"Loading chatbot components...\")\n",
        "\n",
        "# Load all saved components\n",
        "with open('chatbot_751_components.pkl', 'rb') as f:\n",
        "    components = pickle.load(f)\n",
        "\n",
        "# Extract all components\n",
        "vocab = components['vocab']\n",
        "word_to_idx = components['word_to_idx']\n",
        "idx_to_word = components['idx_to_word']\n",
        "questions_processed = components['questions_processed']\n",
        "answers_processed = components['answers_processed']\n",
        "df = components['df']\n",
        "MAX_QUESTION_LENGTH = components['max_question_length']\n",
        "MAX_ANSWER_LENGTH = components['max_answer_length']\n",
        "EMBEDDING_DIM = components['embedding_dim']\n",
        "embedding_matrix = components['embedding_matrix']\n",
        "\n",
        "print(\"Loading trained models...\")\n",
        "\n",
        "# Load all three models (using .keras format)\n",
        "main_model = load_model('market_research_chatbot_751_model.keras')\n",
        "encoder_model = load_model('market_research_encoder_model.keras')\n",
        "decoder_model = load_model('market_research_decoder_model.keras')\n",
        "\n",
        "print(\"Recreating preprocessor...\")\n",
        "\n",
        "# Recreate the TextPreprocessor class\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\\\s]', '', text)\n",
        "        text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"Tokenize text using NLTK\"\"\"\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                 if token not in self.stop_words and len(token) > 1]\n",
        "        return tokens\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        cleaned_text = self.clean_text(text)\n",
        "        tokens = self.tokenize_text(cleaned_text)\n",
        "        return tokens\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "print(\"Setting up inference functions...\")\n",
        "\n",
        "# Similarity search function\n",
        "def similarity_search(query_tokens, questions_processed, threshold=0.2):\n",
        "    \"\"\"Find most similar question using token overlap\"\"\"\n",
        "    best_match_idx = -1\n",
        "    best_score = 0\n",
        "\n",
        "    query_set = set(query_tokens)\n",
        "\n",
        "    for i, q_tokens in enumerate(questions_processed):\n",
        "        q_set = set(q_tokens)\n",
        "\n",
        "        intersection = len(query_set.intersection(q_set))\n",
        "        union = len(query_set.union(q_set))\n",
        "\n",
        "        if union > 0:\n",
        "            score = intersection / union\n",
        "            if score > best_score and score >= threshold:\n",
        "                best_score = score\n",
        "                best_match_idx = i\n",
        "\n",
        "    return best_match_idx, best_score\n",
        "\n",
        "# Sequence decoding function\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model, word_to_idx, idx_to_word, max_decoder_seq_length):\n",
        "    \"\"\"Decode a sequence using the trained model\"\"\"\n",
        "    try:\n",
        "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = word_to_idx['<START>']\n",
        "\n",
        "        decoded_sentence = []\n",
        "        max_iterations = max_decoder_seq_length\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_word = idx_to_word.get(sampled_token_index, '<UNK>')\n",
        "\n",
        "            if sampled_word in ['<END>', '<PAD>'] or len(decoded_sentence) >= max_decoder_seq_length:\n",
        "                break\n",
        "\n",
        "            if sampled_word != '<UNK>':\n",
        "                decoded_sentence.append(sampled_word)\n",
        "\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "            states_value = [h, c]\n",
        "\n",
        "        return ' '.join(decoded_sentence)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in decode_sequence: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Main chatbot response function\n",
        "def get_chatbot_response(user_input):\n",
        "    \"\"\"Get response from the chatbot\"\"\"\n",
        "    try:\n",
        "        user_tokens = preprocessor.preprocess(user_input)\n",
        "\n",
        "        if not user_tokens:\n",
        "            return \"I'm sorry, I couldn't understand your question. Could you please rephrase it?\"\n",
        "\n",
        "        # Try similarity search first\n",
        "        match_idx, similarity_score = similarity_search(user_tokens, questions_processed, threshold=0.2)\n",
        "\n",
        "        if match_idx != -1 and similarity_score > 0.4:\n",
        "            return df.iloc[match_idx]['Answer']\n",
        "\n",
        "        # Use neural network prediction\n",
        "        user_sequence = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in user_tokens]\n",
        "        user_padded = pad_sequences([user_sequence], maxlen=MAX_QUESTION_LENGTH, padding='post')\n",
        "\n",
        "        response = decode_sequence(\n",
        "            user_padded,\n",
        "            encoder_model,\n",
        "            decoder_model,\n",
        "            word_to_idx,\n",
        "            idx_to_word,\n",
        "            MAX_ANSWER_LENGTH\n",
        "        )\n",
        "\n",
        "        if len(response.strip()) < 5:\n",
        "            if match_idx != -1:\n",
        "                return df.iloc[match_idx]['Answer']\n",
        "            else:\n",
        "                return \"I'm sorry, I couldn't find relevant information for your query. Could you try rephrasing your question?\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_chatbot_response: {e}\")\n",
        "        return \"I'm sorry, I encountered an error processing your request. Please try again.\"\n",
        "\n",
        "# Interactive chatbot function\n",
        "def start_chatbot():\n",
        "    \"\"\"Start interactive chatbot session\"\"\"\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\"MARKET RESEARCH CHATBOT - LOADED FROM SAVED MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Ask me anything about market research, startups, and business!\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "            print(\"Bot: Thank you for using the Market Research Chatbot. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input:\n",
        "            response = get_chatbot_response(user_input)\n",
        "            print(f\"Bot: {response}\")\n",
        "        else:\n",
        "            print(\"Bot: Please enter a valid question.\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"CHATBOT LOADED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Training samples: {len(questions_processed)}\")\n",
        "print(f\"Model parameters: {main_model.count_params():,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test the loaded chatbot\n",
        "print(\"\\\\nTesting loaded chatbot...\")\n",
        "test_query = \"What is market research?\"\n",
        "print(f\"Test Query: {test_query}\")\n",
        "print(f\"Response: {get_chatbot_response(test_query)}\")\n",
        "\n",
        "print(\"\\\\nYour chatbot is ready! Use start_chatbot() to begin interactive session.\")\n",
        "print(\"Or use get_chatbot_response('your question') for single queries.\")\n",
        "    '''\n",
        "    return loading_code\n",
        "\n",
        "# Save the complete loading code to a file\n",
        "loading_instructions = load_saved_chatbot()\n",
        "with open('load_chatbot_keras.py', 'wb') as f:\n",
        "    f.write(loading_instructions.encode('utf-8'))\n",
        "\n",
        "print(\"Complete loading function saved to: load_chatbot_keras.py\")\n",
        "print(\"\\\\nTo use your saved chatbot in future:\")\n",
        "print(\"1. Run: exec(open('load_chatbot_keras.py').read())\")\n",
        "print(\"2. Or copy-paste the code from load_chatbot_keras.py\")\n",
        "print(\"3. Then use: get_chatbot_response('your question')\")\n",
        "print(\"4. Or use: start_chatbot() for interactive mode\")"
      ],
      "metadata": {
        "id": "XV0ZYKz8j0ym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}